{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f795fe",
   "metadata": {},
   "source": [
    "# ğŸ“Š ç‰ˆæœ¬å¯¹æ¯”åˆ†ææŠ¥å‘Š\n",
    "\n",
    "> è‡ªåŠ¨å¯¹æ¯”å¤šä¸ªç‰ˆæœ¬çš„æ€§èƒ½ã€è´¨é‡å’Œæˆæœ¬æ•°æ®\n",
    "\n",
    "**ç›®çš„**: å¯è§†åŒ–å±•ç¤ºä¼˜åŒ–æ•ˆæœï¼Œè¾…åŠ©å†³ç­–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾æ ·å¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d5925",
   "metadata": {},
   "source": [
    "## ğŸ“‚ åŠ è½½åŸºå‡†æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98083e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_benchmarks(benchmark_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åŠ è½½æ‰€æœ‰åŸºå‡† JSON æ–‡ä»¶ï¼Œè½¬æ¢ä¸º DataFrame\n",
    "    \"\"\"\n",
    "    benchmark_files = sorted(benchmark_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if not benchmark_files:\n",
    "        print(\"âš ï¸  æœªæ‰¾åˆ°åŸºå‡†æ•°æ®æ–‡ä»¶\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for file in benchmark_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # å±•å¹³åµŒå¥—ç»“æ„\n",
    "        record = {\n",
    "            'version': data['version'],\n",
    "            'timestamp': data['timestamp'],\n",
    "            'file': file.name\n",
    "        }\n",
    "        \n",
    "        # æ€§èƒ½æŒ‡æ ‡\n",
    "        for key, value in data.get('performance', {}).items():\n",
    "            if isinstance(value, dict):\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    record[f'perf_{key}_{sub_key}'] = sub_value\n",
    "            else:\n",
    "                record[f'perf_{key}'] = value\n",
    "        \n",
    "        # è´¨é‡æŒ‡æ ‡\n",
    "        for key, value in data.get('quality', {}).items():\n",
    "            record[f'quality_{key}'] = value\n",
    "        \n",
    "        # æˆæœ¬æŒ‡æ ‡\n",
    "        for key, value in data.get('cost', {}).items():\n",
    "            record[f'cost_{key}'] = value\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "benchmark_dir = Path.cwd()\n",
    "df_benchmarks = load_all_benchmarks(benchmark_dir)\n",
    "\n",
    "if not df_benchmarks.empty:\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df_benchmarks)} ä¸ªç‰ˆæœ¬çš„åŸºå‡†æ•°æ®\")\n",
    "    display(df_benchmarks[['version', 'timestamp', 'file']].head())\n",
    "else:\n",
    "    print(\"â„¹ï¸  è¯·å…ˆè¿è¡Œ eval_framework.ipynb ç”ŸæˆåŸºå‡†æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a97f2",
   "metadata": {},
   "source": [
    "## âš¡ æ€§èƒ½å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_benchmarks.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. ç«¯åˆ°ç«¯å“åº”æ—¶é—´\n",
    "    if 'perf_end_to_end_avg_time' in df_benchmarks.columns:\n",
    "        ax = axes[0, 0]\n",
    "        df_benchmarks.plot(x='version', y='perf_end_to_end_avg_time', \n",
    "                          kind='bar', ax=ax, color='skyblue', legend=False)\n",
    "        ax.set_title('ç«¯åˆ°ç«¯å“åº”æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('æ—¶é—´ (ç§’)', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. LLM å“åº”æ—¶é—´\n",
    "    if 'perf_llm_avg_response_time' in df_benchmarks.columns:\n",
    "        ax = axes[0, 1]\n",
    "        df_benchmarks.plot(x='version', y='perf_llm_avg_response_time', \n",
    "                          kind='bar', ax=ax, color='lightcoral', legend=False)\n",
    "        ax.set_title('LLM å“åº”æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('æ—¶é—´ (ç§’)', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. æ•°æ®è·å–æ—¶é—´\n",
    "    if 'perf_data_fetching_avg_time' in df_benchmarks.columns:\n",
    "        ax = axes[1, 0]\n",
    "        df_benchmarks.plot(x='version', y='perf_data_fetching_avg_time', \n",
    "                          kind='bar', ax=ax, color='lightgreen', legend=False)\n",
    "        ax.set_title('æ•°æ®è·å–æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('æ—¶é—´ (ç§’)', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. æŒ‡æ ‡è®¡ç®—æ—¶é—´ï¼ˆæ€»å’Œï¼‰\n",
    "    indicator_cols = [col for col in df_benchmarks.columns if col.startswith('perf_') and 'times_avg' in col]\n",
    "    if indicator_cols:\n",
    "        ax = axes[1, 1]\n",
    "        df_benchmarks['total_indicator_time'] = df_benchmarks[indicator_cols].sum(axis=1)\n",
    "        df_benchmarks.plot(x='version', y='total_indicator_time', \n",
    "                          kind='bar', ax=ax, color='gold', legend=False)\n",
    "        ax.set_title('æŒ‡æ ‡è®¡ç®—æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('æ—¶é—´ (ç§’)', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd784cc",
   "metadata": {},
   "source": [
    "## âœ… è´¨é‡å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_benchmarks.empty:\n",
    "    quality_cols = [col for col in df_benchmarks.columns if col.startswith('quality_')]\n",
    "    \n",
    "    if quality_cols:\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾\n",
    "        df_quality = df_benchmarks[['version'] + quality_cols].set_index('version')\n",
    "        df_quality.plot(kind='bar', ax=ax, width=0.8)\n",
    "        \n",
    "        ax.set_title('è´¨é‡æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel('è¯„åˆ†/æ¯”ç‡', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='åŠæ ¼çº¿ (0.8)')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('quality_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ… è´¨é‡å¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191b21b",
   "metadata": {},
   "source": [
    "## ğŸ’° æˆæœ¬å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f48582",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_benchmarks.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Token ä½¿ç”¨é‡\n",
    "    if 'cost_avg_total_tokens' in df_benchmarks.columns:\n",
    "        ax = axes[0]\n",
    "        df_benchmarks.plot(x='version', y='cost_avg_total_tokens', \n",
    "                          kind='bar', ax=ax, color='mediumpurple', legend=False)\n",
    "        ax.set_title('å¹³å‡ Token ä½¿ç”¨é‡', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Token æ•°', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "        for i, v in enumerate(df_benchmarks['cost_avg_total_tokens']):\n",
    "            ax.text(i, v + 50, f'{int(v)}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 2. å•æ¬¡æŸ¥è¯¢æˆæœ¬\n",
    "    if 'cost_total_cost_cny' in df_benchmarks.columns:\n",
    "        ax = axes[1]\n",
    "        df_benchmarks.plot(x='version', y='cost_total_cost_cny', \n",
    "                          kind='bar', ax=ax, color='salmon', legend=False)\n",
    "        ax.set_title('å•æ¬¡æŸ¥è¯¢æˆæœ¬', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('æˆæœ¬ (CNY)', fontsize=12)\n",
    "        ax.set_xlabel('ç‰ˆæœ¬', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "        for i, v in enumerate(df_benchmarks['cost_total_cost_cny']):\n",
    "            ax.text(i, v + 0.0001, f'Â¥{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cost_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… æˆæœ¬å¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73faad68",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ±‡æ€»å¯¹æ¯”è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979591a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_benchmarks.empty and len(df_benchmarks) >= 2:\n",
    "    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”\n",
    "    baseline = df_benchmarks.iloc[0]\n",
    "    latest = df_benchmarks.iloc[-1]\n",
    "    \n",
    "    improvements = []\n",
    "    \n",
    "    metrics = [\n",
    "        ('perf_end_to_end_avg_time', 'ç«¯åˆ°ç«¯æ—¶é—´', 's', 'lower'),\n",
    "        ('perf_llm_avg_response_time', 'LLM å“åº”æ—¶é—´', 's', 'lower'),\n",
    "        ('perf_data_fetching_avg_time', 'æ•°æ®è·å–æ—¶é—´', 's', 'lower'),\n",
    "        ('cost_avg_total_tokens', 'Token ä½¿ç”¨', 'ä¸ª', 'lower'),\n",
    "        ('cost_total_cost_cny', 'å•æ¬¡æˆæœ¬', 'å…ƒ', 'lower'),\n",
    "        ('quality_format_correct_rate', 'æ ¼å¼æ­£ç¡®ç‡', '%', 'higher'),\n",
    "        ('quality_content_complete_rate', 'å†…å®¹å®Œæ•´ç‡', '%', 'higher'),\n",
    "    ]\n",
    "    \n",
    "    for metric, name, unit, direction in metrics:\n",
    "        if metric in baseline and metric in latest:\n",
    "            base_val = baseline[metric]\n",
    "            latest_val = latest[metric]\n",
    "            \n",
    "            if base_val > 0:\n",
    "                change_pct = ((latest_val - base_val) / base_val) * 100\n",
    "                \n",
    "                # åˆ¤æ–­æ˜¯æ”¹è¿›è¿˜æ˜¯é€€æ­¥\n",
    "                if direction == 'lower':\n",
    "                    is_improvement = change_pct < 0\n",
    "                    emoji = 'âœ…' if is_improvement else 'âŒ'\n",
    "                else:\n",
    "                    is_improvement = change_pct > 0\n",
    "                    emoji = 'âœ…' if is_improvement else 'âŒ'\n",
    "                \n",
    "                improvements.append({\n",
    "                    'æŒ‡æ ‡': name,\n",
    "                    f'{baseline[\"version\"]}': f'{base_val:.4f} {unit}',\n",
    "                    f'{latest[\"version\"]}': f'{latest_val:.4f} {unit}',\n",
    "                    'å˜åŒ–': f'{change_pct:+.1f}%',\n",
    "                    'çŠ¶æ€': emoji\n",
    "                })\n",
    "    \n",
    "    df_improvements = pd.DataFrame(improvements)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ“Š ä¼˜åŒ–æ•ˆæœæ€»ç»“: {baseline['version']} â†’ {latest['version']}\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_improvements)\n",
    "    \n",
    "    # ä¿å­˜ä¸º CSV\n",
    "    df_improvements.to_csv('improvement_summary.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nâœ… æ±‡æ€»è¡¨å·²ä¿å­˜: improvement_summary.csv\")\n",
    "    \n",
    "elif len(df_benchmarks) == 1:\n",
    "    print(\"â„¹ï¸  åªæœ‰ä¸€ä¸ªç‰ˆæœ¬çš„æ•°æ®ï¼Œæ— æ³•å¯¹æ¯”\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  æš‚æ— æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a1396",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ä¼˜åŒ–å»ºè®®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_benchmarks.empty:\n",
    "    latest = df_benchmarks.iloc[-1]\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # æ€§èƒ½å»ºè®®\n",
    "    if 'perf_llm_avg_response_time' in latest and latest['perf_llm_avg_response_time'] > 3.0:\n",
    "        recommendations.append({\n",
    "            'ç±»å‹': 'æ€§èƒ½',\n",
    "            'ä¼˜å…ˆçº§': 'é«˜',\n",
    "            'å»ºè®®': 'LLM å“åº”æ—¶é—´ > 3sï¼Œå»ºè®®ä¼˜åŒ– Prompt æˆ–å°è¯•æ›´å¿«çš„æ¨¡å‹'\n",
    "        })\n",
    "    \n",
    "    if 'perf_data_fetching_avg_time' in latest and latest['perf_data_fetching_avg_time'] > 1.0:\n",
    "        recommendations.append({\n",
    "            'ç±»å‹': 'æ€§èƒ½',\n",
    "            'ä¼˜å…ˆçº§': 'ä¸­',\n",
    "            'å»ºè®®': 'æ•°æ®è·å–æ—¶é—´ > 1sï¼Œå»ºè®®å®ç°ç¼“å­˜æœºåˆ¶'\n",
    "        })\n",
    "    \n",
    "    # è´¨é‡å»ºè®®\n",
    "    if 'quality_content_complete_rate' in latest and latest['quality_content_complete_rate'] < 0.9:\n",
    "        recommendations.append({\n",
    "            'ç±»å‹': 'è´¨é‡',\n",
    "            'ä¼˜å…ˆçº§': 'é«˜',\n",
    "            'å»ºè®®': 'å†…å®¹å®Œæ•´ç‡ < 90%ï¼Œå»ºè®®ä¼˜åŒ– Prompt æŒ‡ä»¤çš„æ˜ç¡®æ€§'\n",
    "        })\n",
    "    \n",
    "    # æˆæœ¬å»ºè®®\n",
    "    if 'cost_avg_total_tokens' in latest and latest['cost_avg_total_tokens'] > 2000:\n",
    "        recommendations.append({\n",
    "            'ç±»å‹': 'æˆæœ¬',\n",
    "            'ä¼˜å…ˆçº§': 'ä¸­',\n",
    "            'å»ºè®®': 'Token ä½¿ç”¨é‡è¾ƒé«˜ï¼Œå»ºè®®ç²¾ç®€ Prompt æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹'\n",
    "        })\n",
    "    \n",
    "    if recommendations:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ’¡ ä¼˜åŒ–å»ºè®®\")\n",
    "        print(\"=\"*80)\n",
    "        df_recommendations = pd.DataFrame(recommendations)\n",
    "        display(df_recommendations)\n",
    "    else:\n",
    "        print(\"\\nâœ… æ‰€æœ‰æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d2d59",
   "metadata": {},
   "source": [
    "## ğŸ’¾ å¯¼å‡ºæŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ Markdown æŠ¥å‘Š\n",
    "if not df_benchmarks.empty:\n",
    "    report_lines = [\n",
    "        \"# ğŸ“Š ä¼˜åŒ–æ•ˆæœæŠ¥å‘Š\\n\",\n",
    "        f\"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "        f\"**å¯¹æ¯”ç‰ˆæœ¬**: {df_benchmarks.iloc[0]['version']} â†’ {df_benchmarks.iloc[-1]['version']}\\n\\n\",\n",
    "        \"## æ€§èƒ½æ”¹è¿›\\n\",\n",
    "        \"![æ€§èƒ½å¯¹æ¯”](performance_comparison.png)\\n\\n\",\n",
    "        \"## è´¨é‡æ”¹è¿›\\n\",\n",
    "        \"![è´¨é‡å¯¹æ¯”](quality_comparison.png)\\n\\n\",\n",
    "        \"## æˆæœ¬æ”¹è¿›\\n\",\n",
    "        \"![æˆæœ¬å¯¹æ¯”](cost_comparison.png)\\n\\n\",\n",
    "    ]\n",
    "    \n",
    "    if len(df_benchmarks) >= 2:\n",
    "        report_lines.append(\"## æ±‡æ€»æ•°æ®\\n\\n\")\n",
    "        report_lines.append(df_improvements.to_markdown(index=False))\n",
    "    \n",
    "    with open('optimization_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.writelines(report_lines)\n",
    "    \n",
    "    print(\"âœ… æŠ¥å‘Šå·²å¯¼å‡º: optimization_report.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
